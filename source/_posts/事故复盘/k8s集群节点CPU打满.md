---
title: 凌晨k8s某个pod出现大量调用redis的耗时增加日志
permalink: 凌晨k8s某个pod出现大量调用redis的耗时增加日志
sticky: true
cover: https://blog-1259743669.cos.ap-chengdu.myqcloud.com/picgo/abd2551c0de25dd46f17fdd040ac4f92.png
date: 2025-09-09 22:46:36
description: 监控平台skynet出现大量消息堆积，触发告警，排查发现是某项目的调用日志疯狂打印，主要是在集群内内网调用redis、MQ等出现耗时增加，对redis的调用耗时居然达到300ms，这是内网哦，正常情况下 内网调用应该不超过20ms才对
tags:
- [k8s]
- [事故记录]
categories:
- [事故记录]
---

## 起因
监控平台skynet出现大量消息堆积，触发告警，排查发现是某项目的调用日志疯狂打印，主要是在集群内内网调用redis、MQ等出现耗时增加，对redis的调用耗时居然达到300ms，这是内网哦，正常情况下 内网调用应该不超过20ms才对

## 临时修复
发现问题后，马上摇对应的业务方，一起排查，检查redis日志后并没有发现大key，代理节点耗时激增问题，进一步排查业务日志发现，对应超时日志仅在一个pod上出现，我们相同的服务有3个实例， 怀疑是pod内部调用有啥问题，马上干掉pod重新分配。发现新创建的pod依然有问题，怀疑是pod所在节点有问题，然后想办法调度到另外的节点。
临时处理方案是，增加pod数，让k8s在另外节点创建，然后马上删掉一个pod，并快速缩减pod，这样就不会始终在一个节点新建pod了，应该有其他方案，当时没找到。经过操作后，超时日志逐渐减少，第二天深度检查。

## 排查过程
首先检查了对应redis，发现并没有异常和慢日志等，那么说明超时应该是在pod调用redis的阶段，另外向阿里云提交了工单，协助排查。
1. 检查redis日志，大key，调用时延，未发现实现
2. 考虑检查vpc，但是没找到监控位置，
3. 阿里云提供信息判断，在日志增加期间，所处ecs节点cpu占用率打满，可能是主线程阻塞导致的。
4. 后续检查发现，我们各个服务的网关和对cpu占用较高的服务的部分pod都集中到出问题的节点上，导致该节点cpu占用率很高。

## 解决方案
1. 加钱 增加了节点，将部分服务重新部署，分配到新的节点池。降低节点负载
2. 补充对ecs的监控，后续若出现ecs资源占用率较高，可第一时间发现并处理

## 反思与收获
1. 这次虽然没快速定位到了问题，但是误打误撞临时解决了，遇到紧急问题，先看清楚问题，然后快速摇人一起处理
2. 排查方向基本是正确的，但是缺少了一些经验，在一开始就应该先检查每个pod服务和日志，对于仅在一个pod出现异常日志，而其他pod没有 应该考虑，这个pod与其他pod有什么不同，pod所属节点、节点所在可用区，pod负载等；
3. 学习到，节点的负载，会影响该节点所属业务运行效率，所以 对于节点的占用，和日常数据库等cpu，并不是占用率越高越好，而是达到一个相对平衡的状态，既不能负载太高，同时也要兼顾成本，空闲率较高。
